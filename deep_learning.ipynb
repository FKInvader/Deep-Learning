{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315290f-22f8-426e-bce5-3d3c4a79f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# social network analysis\n",
    "import networkx as nx\n",
    "\n",
    "# text preprocessing\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# machine learning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a37ef2-c0cb-4391-aa1b-38059b919e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading in the avatar demoographics dataset\n",
    "# There are characters from the Avatar: LOK which is the second installation in the Universe\n",
    "demo = pd.read_csv('../data/avatar_characters_data.csv', encoding='utf-8', encoding_errors='replace')\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e997c6-97e9-4124-ad41-63ece24c2435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading in the avatar script dataset\n",
    "script = pd.read_csv('../data/avatar.csv', encoding='utf-8', encoding_errors='replace')\n",
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1805a1-fd64-4bfa-bd93-e06d0562234e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of characters found in script dataset. \n",
    "avatar_chars = script['character'].unique()\n",
    "print(avatar_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd21b7f-c382-423e-a2f5-05d2f47a2a21",
   "metadata": {},
   "source": [
    "### Clustering Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b4598-4d80-48ec-86f5-27f8f0523b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making a list of episodes for each Avatar character\n",
    "script_rd = script[['chapter','character']].copy()\n",
    "script_rd = script_rd.drop_duplicates()\n",
    "script_rd = script_rd.groupby('character')['chapter'].apply(list).reset_index()\n",
    "\n",
    "# Removing the Scene Description\n",
    "script_rd = script_rd[script_rd['character'] != 'Scene Description']\n",
    "\n",
    "# To match the naming convention in the other dataset, need to remove the prefixes of certain characters\n",
    "subs = ['Avatar ', 'Young ']\n",
    "pattern = '|'.join(map(re.escape,subs))\n",
    "script_rd['character'] = script_rd['character'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Finding the number of episodes a character is in and removing characters that only appear in 1 episode\n",
    "script_rd['epi_cnt'] = script_rd['chapter'].apply(len)\n",
    "script_rd = script_rd[script_rd['epi_cnt'] > 1].reset_index(drop=True)\n",
    "script_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b043f9a-e75c-47a1-aad7-156f1124e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the jaccard index for each pair of characters to know how often they are in chapters together as a function of overall total of chapters each character was in\n",
    "chars = len(script_rd)\n",
    "sims = []\n",
    "for i in range(chars):\n",
    "    for j in range(i+1, chars):\n",
    "        char1 = set(script_rd.iloc[i]['chapter'])\n",
    "        char2 = set(script_rd.iloc[j]['chapter'])\n",
    "        overlap = char1.intersection(char2)\n",
    "        total = char1.union(char2)\n",
    "        perc = round((len(overlap) / len(total)),2)\n",
    "        res = (script_rd.iloc[i,0], script_rd.iloc[j,0], perc)\n",
    "        sims.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb27d38-5f5c-4505-a7f7-d5ea1e161c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group = pd.DataFrame(sims, columns=['source','target','jaccard'])\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd43eb8-1852-45fd-af34-f87eec07f203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Toph is one of the few characters with a last name and it messes up the naming convention \n",
    "demo['Character Name'] = demo['Character Name'].str.replace('Toph Beifong', 'Toph')\n",
    "\n",
    "# Filtering characters in the demographic datasets by the Avatar characters only list\n",
    "demo_rd = demo[['Character Name','Ethnicity','Weapon of choice', 'First appearance']]\n",
    "\n",
    "demo_rd = demo_rd[demo_rd['Character Name'].isin(avatar_chars)]\n",
    "demo_rd['cln_appearance'] = demo_rd['First appearance'].str.extract(r'\"(.*?)\"')\n",
    "\n",
    "demo_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728bf0e-399e-4d76-acea-875f296f5726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identifying the book for each episode\n",
    "ep_bk = script[['chapter','book', 'book_num']].copy()\n",
    "ep_bk.drop_duplicates(inplace=True)\n",
    "ep_bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711ad9f-a444-4941-8f00-db4ef05e134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_rd = demo_rd.merge(ep_bk, left_on='cln_appearance', right_on='chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee880398-5c21-4a03-a105-6f171a0a6b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identifying the book of the first appearance of each character\n",
    "demo_rd = demo_rd.drop(['chapter', 'First appearance'],axis=1)\n",
    "demo_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ebaf2b-8dc8-4791-b648-d837e51e9a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making the columns to identify if a character pair has matching ethnicity, weapon choice and book of first appearance\n",
    "group['mat_eth'] = None\n",
    "group['mat_weapon'] = None\n",
    "group['mat_book'] = None\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af533c4-e331-4aef-b6f0-34b60d310389",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in group.iterrows():\n",
    "    char1 = row['source']\n",
    "    char2 = row['target']\n",
    "    temp = demo_rd[demo_rd['Character Name'].isin([char1, char2])]\n",
    "    if len(temp) == 1:\n",
    "        continue\n",
    "    group.loc[group.index[index], 'mat_eth'] = 1 if len(set(temp['Ethnicity'])) == 1 else 0\n",
    "    group.loc[group.index[index], 'mat_weapon'] = 1 if len(set(temp['Weapon of choice'])) == 1 else 0\n",
    "    group.loc[group.index[index], 'mat_book'] = 1 if len(set(temp['book'])) == 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93407114-32d7-4c5a-95be-03083adbdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up the values to determine an overall score\n",
    "group['total'] = group[['jaccard','mat_eth','mat_weapon','mat_book']].sum(axis=1)\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fac1e-0e3a-4ef3-831e-78c778aef975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing groups that don't have any matching criteria\n",
    "group = group.dropna(subset='mat_book')\n",
    "group = group[group['total']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c5c8b-aa46-4ad2-b56e-b2d6920ca708",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = group.reset_index(drop=True)\n",
    "group['total'] = group['total'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19945e98-cc53-492c-a26e-f96a8b65b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pairs have the highest scores; looks like side characaters show stronger relationships because they are in few and same episodes\n",
    "max_connections = group.nlargest(n=10,columns='total')\n",
    "max_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62594ab-e8bc-4f27-9b12-278be2876742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of scores; skew on the ones probably due to side characters appearing in two episodes together and having no matching characteristics\n",
    "fig_scores = px.histogram(group,x='total', nbins=8, text_auto=True, template='simple_white')\n",
    "fig_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34854cbd-5ed5-4a89-8507-9287810a62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eps = script_rd.nlargest(n=20,columns='epi_cnt')\n",
    "max_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b65553-3401-42b7-8289-950db5ddef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the network graph to map characters and the strength of their relationship based on how much they match on the characteristics\n",
    "G = nx.from_pandas_edgelist(group, 'source', 'target', 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b6e59-e50e-4325-a0c5-3cee5a7c8fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are two distinct clusters: one that seems to include the main cast and another cluster of side-characters\n",
    "plt.figure(figsize=(18, 14))\n",
    "nx.draw_networkx(G, with_labels=True)\n",
    "plt.title('Network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f15a0-59aa-49d0-8130-0716a8e073a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Community detection to see how groups get clustered. There are four groups which looks roughly like a water tribe based group, a fire nation based group\n",
    "# a group full of other meaningful characters, and a group for everyone else\n",
    "comms = nx.community.louvain_communities(G, weight='total', seed=42)\n",
    "comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46b450-340b-4ac2-9e66-d33560fccdc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# breaking apart each group to see the connections between them\n",
    "community_features = []\n",
    "for i, comm in enumerate(comms):\n",
    "    subgraph = G.subgraph(comm)\n",
    "    plt.figure(figsize=(20, 18))\n",
    "    nx.draw_networkx(subgraph, with_labels=True,node_size=2000, edge_cmap=plt.cm.Greys)\n",
    "    plt.title('Network ' + str(i))\n",
    "    plt.show()\n",
    "    # Example features: average degree, number of edges, number of nodes\n",
    "    avg_degree = sum(dict(subgraph.degree()).values()) / len(subgraph)\n",
    "    num_edges = subgraph.number_of_edges()\n",
    "    num_nodes = subgraph.number_of_nodes()\n",
    "    community_features.append([avg_degree, num_edges, num_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7ca1f-e6fd-4fc0-bbf6-344272828689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the average number of connections for the nodes, number of connections in the graph, number of nodes\n",
    "community_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04034718-2d3d-4aef-95b5-09e364d63b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# break down of ethnicity, book of appearance, histogram of episodes, weapon of choice \n",
    "demo_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd65ed6-b520-488e-9d11-7df17c81266e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "script_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc64a2-81a7-4590-bbc3-c37ac7fe0110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df = script_rd.merge(demo_rd, left_on='character', right_on='Character Name').drop('Character Name',axis=1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816f9c9-30c1-435b-8052-802945eb86d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making graphs to analyzse the clusters to confirm the hypothesis that there is a water, fire, other meaningful, and side characters\n",
    "for each in comms:\n",
    "    df_temp = merged_df[merged_df['character'].isin(each)]\n",
    "    fig_plt = make_subplots(rows=2,cols=2, subplot_titles=('Ethnicity','Weapon of Choice','Book of First Appearance'))\n",
    "    fig_plt.add_trace(go.Histogram(x=df_temp['Ethnicity'],texttemplate= '%{y}', name='Ethnicity Breakdown'),row=1,col=1)\n",
    "    fig_plt.add_trace(go.Histogram(x=df_temp['Weapon of choice'],texttemplate= '%{y}', name='Weapon of Choice Breakdown'),row=1,col=2)\n",
    "    fig_plt.add_trace(go.Histogram(x=df_temp['book'],texttemplate= '%{y}', name='Book Appearance Breakdown'),row=2,col=1)\n",
    "\n",
    "    fig_plt.update_layout(height=800,width=1600,template='simple_white')\n",
    "    \n",
    "    fig_plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c3de7-32b2-496e-be26-aba976714351",
   "metadata": {},
   "source": [
    "### Using Text to make Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fed402-9fa2-4a4d-8296-a7abdfd16568",
   "metadata": {},
   "outputs": [],
   "source": [
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775aeea-c5be-4099-a3ce-21ede57484e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where characters say nothing\n",
    "script_pre = script[['id','book','chapter','character','character_words']].copy()\n",
    "script_pre = script_pre[script_pre['character_words'].notna()]\n",
    "\n",
    "# Cleaning up the titles\n",
    "subs = ['Avatar ', 'Young ', ':']\n",
    "pattern = '|'.join(map(re.escape,subs))\n",
    "script_pre['character'] = script_pre['character'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "script_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8991d2-ad8d-4b28-b2e0-baccab532d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing text using spacy\n",
    "# Removing punctuation, stopwords, lowercase, lemmatize\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tkns = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "            tkns.append(token.lemma_.lower())\n",
    "    return \" \".join(tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a7f69-93df-41e2-8ae0-e572e7f1e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_pre['text_cln'] = script_pre['character_words'].apply(preprocess_text)\n",
    "script_pre['text_cln'] = script_pre['text_cln'].apply(lambda x: ' '.join(word for word in x.split() if len(word) > 1))\n",
    "script_pre['split_text'] = script_pre['text_cln'].str.split()\n",
    "script_pre['word_count'] = script_pre['split_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e10b71-7d0d-4272-bd1a-50f34562ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792312ab-e27d-4ff7-aeb4-764c8cbf66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_grp = script_pre.groupby('character',as_index=False)['word_count'].sum()\n",
    "top_10_char_word = script_grp.nlargest(10, 'word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4286efc-e7b3-48eb-b190-53d2978dc352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_10_grph = px.bar(top_10_char_word, x='character',y='word_count', template='simple_white', text_auto=True, title='Top 10 Characters by Word Count')\n",
    "top_10_grph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491c3c6-7500-4ebc-aa2a-820db923b3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chars_include = list(script_grp[script_grp['word_count'] >= 50]['character'])\n",
    "chars_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a345782-a7e5-4134-9e27-839b7ad5b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_full_ep = script_pre.groupby(['character', 'chapter'],as_index=False)['text_cln'].agg(' '.join)\n",
    "script_full_ep_lm = script_full_ep[script_full_ep['character'].isin(chars_include)]\n",
    "script_full_ep_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a5ee9-52b5-44ee-bc13-0ec8d1ae8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = CountVectorizer(max_df=0.95, min_df=2)\n",
    "tf = tf_vect.fit_transform(script_full_ep_lm['text_cln'])\n",
    "tf_feature_names = tf_vect.get_feature_names_out()\n",
    "\n",
    "no_topics = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4defa-c025-4131-89c0-24c082f5f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online',random_state=42)\n",
    "lda_output = lda_model.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed30d9-2a8b-40cd-a2e7-b127c56e0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = [f\"Topic_{i+1}\" for i in range(lda_model.n_components)]\n",
    "df_topic = pd.DataFrame(lda_output, columns=topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74a77c-7961-4fc1-bb96-59d0a8c1c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_full_ep_lm.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab00c9-541d-45b2-ab25-81d3372b301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_topic = pd.concat([script_full_ep_lm, df_topic],axis=1)\n",
    "df_char_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fd990-4baf-4c89-9507-6e4d0ecd3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9eb89e-b055-4d2f-bdca-980c6317f74b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_topics(lda_model, tf_feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4e7df-abbf-4f63-9aa4-91cbaf68e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_topic_mod = (df_char_topic.iloc[:,3:] < .10).astype(float).replace({1: 0, 0: 1})\n",
    "df_char_topic_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ac8d3-22aa-40c6-a1a4-10d3a0c0f046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_char_topic[['character','chapter']],df_char_topic_mod],axis=1)\n",
    "df_final['topic_sum'] = df_final.iloc[:,2:].sum(axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1393ce0-cbf0-4b9c-802e-41376cc312cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_grp = df_final.groupby('character', as_index=False)['topic_sum'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1424e4-67cb-4014-89b1-e34e46f94b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic_sum = top_topic_grp.nlargest(10,'topic_sum', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f37650-cd96-4b98-9a79-5ec4db126cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_topic_grph = px.bar(top_topic_sum, x='character', y='topic_sum', template='simple_white', text_auto=True, title='Top 10 Characters by Topic Occurrence Count')\n",
    "top_topic_grph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8ec9d-5e17-4f6f-bb4d-b6a4e1b41593",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_topics = []\n",
    "for each in df_final['character'].unique():\n",
    "    temp = df_final[df_final['character'] == each]\n",
    "    summed = temp.iloc[:,2:52].sum()\n",
    "    topics = list(summed[summed >0].index)\n",
    "    res = (each, topics)\n",
    "    uni_topics.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a788378-8dcf-4a06-aead-560727080d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uni = pd.DataFrame(uni_topics, columns=['character','topics'])\n",
    "df_uni['num_uni_topics'] = df_uni['topics'].apply(len)\n",
    "df_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89729a72-c794-4014-99ef-f8fdb042422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_topics_grph = px.bar(df_uni.nlargest(10,'num_uni_topics'), x='character', y='num_uni_topics', template='simple_white', text_auto=True, title='Top Ten Characters by Unique Topic Count')\n",
    "uni_topics_grph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46446ddc-d310-435f-8141-96c88d6992de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_net = df_uni[['character', 'topics']].explode('topics')\n",
    "df_net = pd.merge(df_net,df_net, on='topics')\n",
    "df_net = df_net[df_net['character_x'] != df_net['character_y']]\n",
    "df_net_grp = df_net.groupby(['character_x','character_y'], as_index=False)['topics'].count()\n",
    "df_net_grp.columns = ['source', 'target','topics']\n",
    "df_net_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378793a4-df9a-413b-aac9-21ee29a7a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_hist_grph = px.histogram(df_net_grp, x='topics', text_auto=True, template='simple_white', title='Distribution of Topics by Pairs')\n",
    "topic_hist_grph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d25a6-c1dc-4ad8-a7a2-384107e7d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = nx.from_pandas_edgelist(df_net_grp[df_net_grp['topics'] >=3], source='source',target='target', edge_attr='topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63fb26-97c9-4684-bbc1-03499a57a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))\n",
    "nx.draw_networkx(K, with_labels=True,node_size=2000, edge_cmap=plt.cm.Greys)\n",
    "plt.title('Network of Connections for Nodes with at least 3 Matched Topics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025fde8-047a-4420-b773-ac2ae9a92e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = nx.from_pandas_edgelist(df_net_grp[df_net_grp['topics'] >=2], source='source',target='target', edge_attr='topics')\n",
    "comms = nx.community.louvain_communities(K, weight='total', seed=42)\n",
    "comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e65e0-b40b-4195-a1f8-a065b9c5ef87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# breaking apart each group to see the connections between them\n",
    "community_features = []\n",
    "for i, comm in enumerate(comms):\n",
    "    subgraph = K.subgraph(comm)\n",
    "    plt.figure(figsize=(20, 18))\n",
    "    nx.draw_networkx(subgraph, with_labels=True,node_size=2000, edge_cmap=plt.cm.Greys)\n",
    "    plt.title('Network ' + str(i))\n",
    "    plt.show()\n",
    "    # Example features: average degree, number of edges, number of nodes\n",
    "    avg_degree = round(sum(dict(subgraph.degree()).values()) / len(subgraph),0)\n",
    "    num_edges = subgraph.number_of_edges()\n",
    "    num_nodes = subgraph.number_of_nodes()\n",
    "    community_features.append([avg_degree, num_edges, num_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e53e0-f936-48aa-9ea8-90ebde2c3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the average number of connections for the nodes, number of connections in the graph, number of nodes\n",
    "community_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a9757-9317-4304-9ffc-e81c6c99f6ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making graphs to analyzse the clusters to confirm the hypothesis that there is a water, fire, other meaningful, and side characters. Some characters aren't found in both datasets\n",
    "for each in comms:\n",
    "    df_temp = merged_df[merged_df['character'].isin(each)]\n",
    "    fig_plt = make_subplots(rows=2,cols=2, subplot_titles=('Ethnicity','Weapon of Choice','Book of First Appearance'))\n",
    "    fig_plt.add_trace(go.Histogram(x=df_temp['Ethnicity'],texttemplate= '%{y}', name='Ethnicity Breakdown'),row=1,col=1)\n",
    "    fig_plt.add_trace(go.Histogram(x=df_temp['Weapon of choice'],texttemplate= '%{y}', name='Weapon of Choice Breakdown'),row=1,col=2)\n",
    "    fig_plt.add_trace(go.Histogram(x=df_temp['book'],texttemplate= '%{y}', name='Book Appearance Breakdown'),row=2,col=1)\n",
    "\n",
    "    fig_plt.update_layout(height=800,width=1600,template='simple_white')\n",
    "    \n",
    "    fig_plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324392b-a1ae-4b0d-b1da-adfc56b4b3a2",
   "metadata": {},
   "source": [
    "## Deep Learning to classify character lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6136397-8bbf-4e1a-a55c-b683a2a7a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_full_ep_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ec097-650d-4ab6-b73d-7f45307b2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_dl = ['Sokka','Toph','Aang','Katara','Suki','Zuko','Azula','Iroh','Ty Lee','Mai','Ozai']\n",
    "script_dl = script_full_ep_lm[script_full_ep_lm['character'].isin(chars_dl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f11cb-ab42-45b6-92f2-aa82ece86e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d4eca-a0c8-4390-993f-da3fba166b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(script_dl['text_cln'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, script_dl['character'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Model Training\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 4. Prediction and Evaluation\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddf5f6-3255-4d52-8f54-6090353f529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_dl.sort()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=chars_dl)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6eb09-f73f-4015-be6b-0bf989360b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Using each line instead of using episodes\n",
    "ln_dl = script_pre[script_pre['character'].isin(chars_dl)]\n",
    "ln_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058c062-85db-4faf-9dde-1d61877a1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(ln_dl['text_cln'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ln_dl['character'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Model Training\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 4. Prediction and Evaluation\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb09b4-12aa-4278-a390-78a2922bfe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_dl.sort()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=chars_dl)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
